{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vladIbV_qil6",
        "4VgZTzB0qncZ",
        "4B6JrahTrGsh",
        "xFVgppityAXh",
        "7m5V6cFIyRZA",
        "QHm1AwyuypjX",
        "NypfPuEBy--o"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for Digestion Diseases Prediction using Word Embeddings and Convolutional Neural Network"
      ],
      "metadata": {
        "id": "ZTvlKg1azQuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "vladIbV_qil6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PpYminqFqGYU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing Functions"
      ],
      "metadata": {
        "id": "4VgZTzB0qncZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data() -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Get pandas.DataFrame for diseases data.\n",
        "\n",
        "  Params : None\n",
        "\n",
        "  Return : pandas.DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  try :\n",
        "    df = pd.read_csv('Digestion Diseases Symptoms.csv')\n",
        "  except Exception as e :\n",
        "    print(e)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "TDso8Pj1qX7_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_list(dataframe: pd.DataFrame, col: str) -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Convert values to list in a column from string with valid list format\n",
        "  (start and end with [ and ] respectively.)\n",
        "\n",
        "  Params :\n",
        "  dataframe : pandas.DataFrame\n",
        "  col : str -> string of column name\n",
        "\n",
        "  Return : pandas.DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  try :\n",
        "    dataframe[col] = dataframe[col].apply(lambda x: ast.literal_eval(x))\n",
        "    if type(dataframe[col].values[0]) == str :\n",
        "      dataframe[col] = dataframe[col].apply(lambda x: ast.literal_eval(x))\n",
        "  except Exception as e :\n",
        "    print(e)\n",
        "\n",
        "  return dataframe"
      ],
      "metadata": {
        "id": "VQmli0VJqfdO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(dataframe: pd.DataFrame, col_of_list: str,\n",
        "           label_col: str, num_sample: int = 5, n: int = 5,\n",
        "           random_state: int = 1) -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Sample randomly from list for every record. Column col_of_list in the dataframe\n",
        "  must have list data type as the values.\n",
        "\n",
        "  Params :\n",
        "  1. pandas.DataFrame\n",
        "\n",
        "  2. col_of_list (column name) : str -> Column in the dataframe that has list as its values.\n",
        "  For example : dataframe.loc[0, col_of_list] = [a,b,c]\n",
        "\n",
        "  3. label_col (column name) : str -> Class column in the dataframe.\n",
        "\n",
        "  4. num_sample : int -> How many samples to generate.\n",
        "  For example, num_samples = 3 and a record has the list [a,b,c]. Then,\n",
        "  the list will be sampled 3 times, generating new 2 records for the same class.\n",
        "\n",
        "  5. n : int -> How many values for each sample.\n",
        "\n",
        "  6. random_state : int -> Integer for random seed for reproducibility.\n",
        "\n",
        "  Return : pandas.DataFrame\n",
        "\n",
        "  \"\"\"\n",
        "  np.random.seed(random_state)\n",
        "  samples, labels = [], []\n",
        "\n",
        "  try :\n",
        "    col_of_list_index = dataframe.columns.to_list().index(col_of_list)\n",
        "    label_col_index = dataframe.columns.to_list().index(label_col)\n",
        "\n",
        "    for record_num in range(len(dataframe)) :\n",
        "      record_list = dataframe.iloc[record_num, col_of_list_index]\n",
        "      record_label = dataframe.iloc[record_num, label_col_index]\n",
        "      if len(record_list) > n :\n",
        "        for _ in range(num_sample):\n",
        "          samples.append(np.random.choice(record_list, n, replace=False))\n",
        "          labels.append(record_label)\n",
        "      else :\n",
        "        for _ in range(num_sample):\n",
        "          samples.append(np.random.choice(record_list, len(record_list)-1, replace=False))\n",
        "          labels.append(record_label)\n",
        "\n",
        "    new_df = pd.DataFrame(list(zip(samples, labels)), columns = dataframe.columns)\n",
        "    return new_df\n",
        "\n",
        "  except Exception as e :\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "taXIaNsyqrW_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_merged_data(dataframe: pd.DataFrame, col_of_list: str,\n",
        "                     label_col: str, num_samples: list,\n",
        "                     n_per_samples: list, random_state: int = 1) -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Make a pandas DataFrame that contains concatenated pandas DataFrames that have been\n",
        "  sampled.\n",
        "\n",
        "  Params :\n",
        "  1. dataframe : pandas.DataFrame\n",
        "\n",
        "  2. col_of_list (column name) : str -> Column in the dataframe that has list as its values.\n",
        "  For example : dataframe.loc[0, col_of_list] = [a,b,c]\n",
        "\n",
        "  3. label_col (column name) : str -> Class column in the dataframe.\n",
        "\n",
        "  4. num_samples : list -> List consists of number of samples that wanted to be generated.\n",
        "  For example, [5,4,3] means that the function will return pandas.DataFrame that is the concatenated\n",
        "  of 3 pandas.DataFrame, upsampled 5, 4, and 3 respectively.\n",
        "\n",
        "  5. n_per_samples : list -> List consists of number of values for each sample in corresponding\n",
        "  pandas.DataFrame that has been upsampled based on num_samples. For example, if num_samples = [5,4,3]\n",
        "  and n_per_samples = [4,3,2], this means that the function will return the concatenated pandas.DataFrame\n",
        "  which consists of these : 5x upsampled pandas.Dataframe, each sample with 4 values/elements, etc.\n",
        "\n",
        "  6. random_state : int -> Integer for random seed for reproducibility.\n",
        "\n",
        "  Return : pandas.DataFrame -> Concatenated dataframe.\n",
        "  \"\"\"\n",
        "\n",
        "  datasets = []\n",
        "\n",
        "  try :\n",
        "    for num_sample, n in zip(num_samples, n_per_samples):\n",
        "      df_sampled = sample(dataframe, col_of_list, label_col, num_sample, n, random_state)\n",
        "      datasets.append(df_sampled)\n",
        "    df_concat = pd.concat(datasets).sort_values(by=label_col).reset_index(drop=True)\n",
        "    return df_concat\n",
        "  except Exception as e :\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "wS5fsVuzquXP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle(dataframe: pd.DataFrame, random_state: int = 1) -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Shuffle the pandas.DataFrame\n",
        "\n",
        "  Params :\n",
        "\n",
        "  1. dataframe : pandas.DataFrame\n",
        "  2. random_state : int -> Random seed for reproducibility.\n",
        "\n",
        "  Return : pandas.DataFrame -> Shuffled pandas.DataFrame.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  try :\n",
        "    new_df = dataframe.sample(len(dataframe), random_state=random_state)\n",
        "    new_df = new_df.reset_index(drop=True)\n",
        "    return new_df\n",
        "  except Exception as e :\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "3vZGsdcwqxam"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_numerical_label(dataframe: pd.DataFrame, labels_col: str) -> tuple[pd.DataFrame, dict] :\n",
        "  \"\"\"\n",
        "  Give numerical labels for the class column\n",
        "\n",
        "  Params :\n",
        "\n",
        "  1. dataframe : pd.DataFrame\n",
        "\n",
        "  2. labels_col : str -> name of label column.\n",
        "\n",
        "  Return :\n",
        "\n",
        "  1. pandas.DataFrame -> Annotated pandas.DataFrame\n",
        "\n",
        "  2. col_dict : dict -> Dictionary that saves labels in integer as the key and the real labels as the values.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    labels = dataframe[labels_col].unique()\n",
        "    col_dict = {key:value for key, value in zip(list(range(len(labels))), labels)}\n",
        "    reverse_dict = {value:key for key, value in col_dict.items()}\n",
        "    dataframe[\"Label\"] = dataframe[labels_col].apply(lambda x: reverse_dict[x])\n",
        "    return dataframe, col_dict\n",
        "  except Exception as e :\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "EnWg2FEAqz7h"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(dataframe: pd.DataFrame, stratify_col: str,\n",
        "          test_size: float = 0.2, random_state: int = 1) -> tuple[pd.DataFrame, pd.DataFrame] :\n",
        "  \"\"\"\n",
        "  Split the DataFrame.\n",
        "\n",
        "  Params :\n",
        "  1. dataframe : pandas.DataFrame\n",
        "  2. stratify_col : str -> Column name to be stratified in train_test_split.\n",
        "  3. test_size : float -> Percentage of test set in floating point.\n",
        "  4. random_state : int -> Random seed for reproducibility.\n",
        "\n",
        "  Return : (pandas.DataFrame, pandas.DataFrame) -> train set and test set.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    dataframe_train, dataframe_test = train_test_split(dataframe, test_size=test_size,\n",
        "                                                       random_state=random_state, stratify=dataframe[stratify_col])\n",
        "    return dataframe_train, dataframe_test\n",
        "  except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "fiqO0Ytlq2Zz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_sentences(dataframe: pd.DataFrame, col_of_list: str) -> pd.DataFrame :\n",
        "  \"\"\"\n",
        "  Convert list values in a column of the DataFrame to string.\n",
        "\n",
        "  Params :\n",
        "  1. dataframe : pandas.DataFrame\n",
        "  2. col_of_list : str -> Column name that has list data type for its values.\n",
        "\n",
        "  Return : pandas.DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  try :\n",
        "    dataframe[col_of_list] = dataframe[col_of_list].apply(lambda x: \", \".join(x))\n",
        "    return dataframe\n",
        "  except Exception as e :\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "MZdo-FIhq6RB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Gathering Functions"
      ],
      "metadata": {
        "id": "4B6JrahTrGsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_data(col_of_list: str,\n",
        "                        label_col: str, num_samples: list,\n",
        "                        n_per_samples: list,\n",
        "                        random_state: int = 1) -> tuple[tuple, tuple, dict] :\n",
        "    \"\"\"\n",
        "    Get train and test data.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. col_of_list : str -> Column name that has list data type for its values.\n",
        "\n",
        "    2. label_col : str -> Name of label column.\n",
        "\n",
        "    3. num_samples : list -> List consists of number of samples that wanted to be generated.\n",
        "    For example, [5,4,3] means that the function will return pandas.DataFrame that is the concatenated\n",
        "    of 3 pandas.DataFrame, upsampled 5, 4, and 3 respectively.\n",
        "\n",
        "    4. n_per_samples : list -> List consists of number of values for each sample in corresponding\n",
        "    pandas.DataFrame that has been upsampled based on num_samples. For example, if num_samples = [5,4,3]\n",
        "    and n_per_samples = [4,3,2], this means that the function will return the concatenated pandas.DataFrame\n",
        "    which consists of these : 5x upsampled pandas.Dataframe, each sample with 4 values/elements, etc.\n",
        "\n",
        "    5. random_state : int -> Integer for random seed for reproducibility.\n",
        "\n",
        "    Return :\n",
        "\n",
        "    tuple -> Consists of 3 elements :\n",
        "\n",
        "    1. train_data : tuple of train sentences and train labels,\n",
        "\n",
        "    2. test_data : tuple of test sentences and test labels,\n",
        "\n",
        "    3. col_dict : dict -> Dictionary that saves labels in integer as the key and the real labels as the values.\n",
        "    \"\"\"\n",
        "    df = get_data()\n",
        "    df = str_to_list(df, col_of_list)\n",
        "    df = make_merged_data(df, col_of_list, label_col,\n",
        "                        num_samples, n_per_samples, random_state)\n",
        "    df = shuffle(df, random_state)\n",
        "    df, col_dict = give_numerical_label(df, label_col)\n",
        "    df = list_to_sentences(df, col_of_list)\n",
        "    train_df, test_df = split(df, \"Label\")\n",
        "\n",
        "    train_sentences = train_df[col_of_list].values\n",
        "    test_sentences = test_df[col_of_list].values\n",
        "    train_labels = train_df[\"Label\"].values\n",
        "    test_labels = test_df[\"Label\"].values\n",
        "\n",
        "    train_data = (train_sentences, train_labels)\n",
        "    test_data = (test_sentences, test_labels)\n",
        "\n",
        "    return train_data, test_data, col_dict"
      ],
      "metadata": {
        "id": "x8CPIyeAx6MV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_sequences(data: tuple, vocab_size: int = 1000,\n",
        "                             max_length: int = 88, oov_tok: str = \"<UNK>\",\n",
        "                             padding_type: str = \"post\", trunc_type: str = \"post\") -> tuple[tuple, tuple, dict] :\n",
        "    \"\"\"\n",
        "    Get training and testing sequences with their labels.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. data : tuple -> Consists of train_data and test_data, each is tuple with 2 elements,\n",
        "    the sentences and the labels.\n",
        "\n",
        "    2. vocab_size : int -> Maximum number of words the tokenizer could save.\n",
        "\n",
        "    3. max_length : int -> Maximum length of a sentence that the tokenizer could save.\n",
        "\n",
        "    4. oov_tok : str -> Out-of-vocab token for unseen words in training data.\n",
        "\n",
        "    5. padding_type : str -> Type of padding in pad_sequences.\n",
        "\n",
        "    6. trunc_type : str -> Type of truncating in pad_sequences.\n",
        "\n",
        "    Return :\n",
        "\n",
        "    tuple -> Consists of 3 elements :\n",
        "\n",
        "    1. train_data : tuple of train sequences and train labels,\n",
        "\n",
        "    2. test_data : tuple of test sequences and test labels\n",
        "\n",
        "    3. word_index : dict -> Dictionary that saves word index from the tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    (train_sentences, train_labels), (test_sentences, test_labels) = data\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "\n",
        "    train_sequences = pad_sequences(train_sequences, maxlen=max_length,\n",
        "                                    padding=padding_type, truncating=trunc_type)\n",
        "    test_sequences = pad_sequences(test_sequences, maxlen=max_length,\n",
        "                                   padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_data = train_sequences, train_labels\n",
        "    test_data = test_sequences, test_labels\n",
        "    return train_data, test_data, tokenizer.word_index"
      ],
      "metadata": {
        "id": "PRZJUl_Zx6yb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building Functions"
      ],
      "metadata": {
        "id": "xFVgppityAXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size: int = 1000, embedding_dim: int = 32,\n",
        "                max_length: int = 88, print_summary: bool = False) -> tf.keras.models.Model :\n",
        "    \"\"\"\n",
        "    Building Deep Learning model for text classification.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. vocab_size : int -> Maximum number of words the tokenizer could save.\n",
        "\n",
        "    2. embedding_dim : int -> Dimension of word embedding used in Embedding Layer.\n",
        "\n",
        "    3. max_length : int -> Maximum length of a sentence that the tokenizer could save.\n",
        "\n",
        "    4. print_summary : bool -> Print model summary if True.\n",
        "\n",
        "    Return : tf.keras.models.Model\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Conv1D(filters=8, kernel_size=8, padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Conv1D(filters=16, kernel_size=16, activation='relu'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=14, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    if print_summary :\n",
        "        model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "Gl6I6OiKx9QQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model: tf.keras.models.Model,\n",
        "                  optimizer: tf.keras.optimizers.Optimizer,\n",
        "                  loss: tf.keras.losses.Loss,\n",
        "                  metrics: list) :\n",
        "    \"\"\"\n",
        "    Compile the model with optimizer, loss, and metrics.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. model : tf.keras.models.Model\n",
        "\n",
        "    2. optimizer : tf.keras.optimizers.Optimizer\n",
        "\n",
        "    3. loss : tf.keras.losses.Loss\n",
        "\n",
        "    4. metrics : list -> Consists of strings that represents metrics' names that\n",
        "    should be displayed while training.\n",
        "\n",
        "    Return : None\n",
        "    \"\"\"\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=loss,\n",
        "                  metrics=metrics)"
      ],
      "metadata": {
        "id": "LdsPrH33yIio"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_callback(threshold: float = 0.98) -> tf.keras.callbacks.Callback :\n",
        "    \"\"\"\n",
        "    Make custom callback that stop training where the metrics have reached certain threshold.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. threshold : float -> threshold of the metrics in floating point percentage.\n",
        "\n",
        "    Return : tf.keras.callbacks.Callback\n",
        "    \"\"\"\n",
        "    class myCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            if logs.get('accuracy') >= threshold and logs.get('val_accuracy') >= threshold :\n",
        "                self.model.stop_training = True\n",
        "\n",
        "    mycallback = myCallback()\n",
        "    return mycallback"
      ],
      "metadata": {
        "id": "hnGF1c_iyKtb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model: tf.keras.models.Model, data: tuple, epochs: int = 1000,\n",
        "             use_callback: bool = True) -> tf.keras.callbacks.History :\n",
        "    \"\"\"\n",
        "    Training the model.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. model : tf.keras.models.Model\n",
        "\n",
        "    2. data : tuple -> Consists of train_data and test_data, each is tuple with 2 elements,\n",
        "    the sentences and the labels.\n",
        "\n",
        "    3. epochs : int -> Number of epochs for training.\n",
        "\n",
        "    4. use_callback : bool -> Use the custom callback if True.\n",
        "\n",
        "    Return : tf.keras.callbacks.History\n",
        "    \"\"\"\n",
        "    (train_data, train_labels), validation_data = data\n",
        "\n",
        "    if use_callback :\n",
        "        callback = make_callback()\n",
        "        history = model.fit(train_data, train_labels, epochs=epochs,\n",
        "                            validation_data=validation_data, callbacks=[callback])\n",
        "    else :\n",
        "        history = model.fit(train_data, train_labels, epochs=epochs,\n",
        "                            validation_data=validation_data)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "snXVK7XEyMq7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model: tf.keras.models.Model) :\n",
        "    model.save('model.h5')"
      ],
      "metadata": {
        "id": "VxKT_bHe1GJc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "7m5V6cFIyRZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training():\n",
        "    train_data, test_data, col_dict = get_train_test_data(\"Gejala\", \"Penyakit\",\n",
        "                                                        [20]*7, [8,7,6,5,4,3,2])\n",
        "\n",
        "    train_sequenced, test_sequenced, word_index = get_train_test_sequences((train_data, test_data))\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    compile_model(model, tf.keras.optimizers.Adam(),\n",
        "                tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                ['accuracy'])\n",
        "\n",
        "    history = training(model, (train_sequenced, test_sequenced))\n",
        "    print(\"\\nTraining accuracy = {:.2f} %\\nTesting accuracy = {:.2f} %\".format(history.history['accuracy'][-1]*100,\n",
        "                                                                               history.history['val_accuracy'][-1]*100))\n",
        "\n",
        "    save_model(model)\n",
        "\n",
        "    with open('word_index.json', 'w') as words:\n",
        "        json.dump(word_index, words)\n",
        "\n",
        "    with open('label_dict.json', 'w') as labels_dict:\n",
        "        json.dump(col_dict, labels_dict)"
      ],
      "metadata": {
        "id": "vK05d9pkyNPp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training()"
      ],
      "metadata": {
        "id": "1E_UxgGsyZH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41441e82-afd6-4a48-9303-4ef1ad73fd07"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "49/49 [==============================] - 2s 12ms/step - loss: 2.6347 - accuracy: 0.0784 - val_loss: 2.6218 - val_accuracy: 0.1020\n",
            "Epoch 2/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 2.5357 - accuracy: 0.1116 - val_loss: 2.3679 - val_accuracy: 0.1505\n",
            "Epoch 3/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 2.1472 - accuracy: 0.2168 - val_loss: 1.8016 - val_accuracy: 0.3418\n",
            "Epoch 4/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 1.5542 - accuracy: 0.4630 - val_loss: 1.1129 - val_accuracy: 0.6913\n",
            "Epoch 5/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 1.0584 - accuracy: 0.6562 - val_loss: 0.7446 - val_accuracy: 0.8520\n",
            "Epoch 6/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.7682 - accuracy: 0.7570 - val_loss: 0.5003 - val_accuracy: 0.8929\n",
            "Epoch 7/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.6051 - accuracy: 0.8202 - val_loss: 0.3813 - val_accuracy: 0.9184\n",
            "Epoch 8/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.4655 - accuracy: 0.8667 - val_loss: 0.2905 - val_accuracy: 0.9311\n",
            "Epoch 9/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.3914 - accuracy: 0.8865 - val_loss: 0.2492 - val_accuracy: 0.9439\n",
            "Epoch 10/1000\n",
            "49/49 [==============================] - 1s 11ms/step - loss: 0.3398 - accuracy: 0.9082 - val_loss: 0.2083 - val_accuracy: 0.9490\n",
            "Epoch 11/1000\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.2865 - accuracy: 0.9247 - val_loss: 0.1789 - val_accuracy: 0.9643\n",
            "Epoch 12/1000\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.2570 - accuracy: 0.9324 - val_loss: 0.1700 - val_accuracy: 0.9566\n",
            "Epoch 13/1000\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.2161 - accuracy: 0.9394 - val_loss: 0.1497 - val_accuracy: 0.9617\n",
            "Epoch 14/1000\n",
            "49/49 [==============================] - 1s 13ms/step - loss: 0.2056 - accuracy: 0.9426 - val_loss: 0.1398 - val_accuracy: 0.9719\n",
            "Epoch 15/1000\n",
            "49/49 [==============================] - 1s 12ms/step - loss: 0.2003 - accuracy: 0.9515 - val_loss: 0.1326 - val_accuracy: 0.9694\n",
            "Epoch 16/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.1847 - accuracy: 0.9598 - val_loss: 0.1332 - val_accuracy: 0.9643\n",
            "Epoch 17/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.1842 - accuracy: 0.9477 - val_loss: 0.1155 - val_accuracy: 0.9745\n",
            "Epoch 18/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.1575 - accuracy: 0.9585 - val_loss: 0.1136 - val_accuracy: 0.9719\n",
            "Epoch 19/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.1214 - accuracy: 0.9707 - val_loss: 0.1090 - val_accuracy: 0.9719\n",
            "Epoch 20/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.1335 - accuracy: 0.9662 - val_loss: 0.1063 - val_accuracy: 0.9668\n",
            "Epoch 21/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.1322 - accuracy: 0.9624 - val_loss: 0.1044 - val_accuracy: 0.9694\n",
            "Epoch 22/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.1186 - accuracy: 0.9675 - val_loss: 0.0914 - val_accuracy: 0.9694\n",
            "Epoch 23/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.1175 - accuracy: 0.9726 - val_loss: 0.0883 - val_accuracy: 0.9821\n",
            "Epoch 24/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.1046 - accuracy: 0.9688 - val_loss: 0.0894 - val_accuracy: 0.9694\n",
            "Epoch 25/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.1133 - accuracy: 0.9681 - val_loss: 0.0876 - val_accuracy: 0.9719\n",
            "Epoch 26/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0989 - accuracy: 0.9732 - val_loss: 0.0852 - val_accuracy: 0.9745\n",
            "Epoch 27/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0818 - accuracy: 0.9783 - val_loss: 0.0785 - val_accuracy: 0.9770\n",
            "Epoch 28/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0939 - accuracy: 0.9764 - val_loss: 0.0813 - val_accuracy: 0.9745\n",
            "Epoch 29/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9758 - val_loss: 0.0792 - val_accuracy: 0.9821\n",
            "Epoch 30/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0915 - accuracy: 0.9764 - val_loss: 0.0733 - val_accuracy: 0.9745\n",
            "Epoch 31/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9713 - val_loss: 0.0767 - val_accuracy: 0.9770\n",
            "Epoch 32/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0964 - accuracy: 0.9732 - val_loss: 0.0740 - val_accuracy: 0.9770\n",
            "Epoch 33/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0899 - accuracy: 0.9745 - val_loss: 0.0692 - val_accuracy: 0.9796\n",
            "Epoch 34/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0918 - accuracy: 0.9700 - val_loss: 0.0754 - val_accuracy: 0.9745\n",
            "Epoch 35/1000\n",
            "49/49 [==============================] - 0s 7ms/step - loss: 0.0852 - accuracy: 0.9790 - val_loss: 0.0654 - val_accuracy: 0.9847\n",
            "Epoch 36/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9770 - val_loss: 0.0616 - val_accuracy: 0.9847\n",
            "Epoch 37/1000\n",
            "49/49 [==============================] - 0s 8ms/step - loss: 0.0701 - accuracy: 0.9821 - val_loss: 0.0611 - val_accuracy: 0.9821\n",
            "\n",
            "Training accuracy = 98.21 %\n",
            "Testing accuracy = 98.21 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Function Helper"
      ],
      "metadata": {
        "id": "QHm1AwyuypjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(string: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove punctuations from string.\n",
        "\n",
        "    Param :\n",
        "\n",
        "    1. string : str\n",
        "\n",
        "    Return : str\n",
        "    \"\"\"\n",
        "    new_str = re.sub(r'[^\\w\\s]', ' ', string)\n",
        "    return new_str"
      ],
      "metadata": {
        "id": "5xnxPlf4ye2l"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model() -> tf.keras.models.Model :\n",
        "    \"\"\"\n",
        "    Load the model.\n",
        "\n",
        "    Return : tf.keras.models.Model\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.load_model('model.h5')\n",
        "    return model"
      ],
      "metadata": {
        "id": "th-zGNfLysxZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_index() -> dict:\n",
        "    \"\"\"\n",
        "    Get the word index.\n",
        "\n",
        "    Return : dict -> The word index.\n",
        "    \"\"\"\n",
        "    with open('word_index.json', 'r') as word_index :\n",
        "        words = json.load(word_index)\n",
        "    return words"
      ],
      "metadata": {
        "id": "dzXAsOrLyuUX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_dict() -> dict:\n",
        "    \"\"\"\n",
        "    Get the label dictionary to translate the prediction from integer to string.\n",
        "\n",
        "    Return : dict -> The label dictionary.\n",
        "    \"\"\"\n",
        "    with open('label_dict.json', 'r') as labels :\n",
        "        label_dict = json.load(labels)\n",
        "    new_label_dict = {}\n",
        "    for key, value in label_dict.items() :\n",
        "        new_label_dict[int(key)] = value\n",
        "    return new_label_dict"
      ],
      "metadata": {
        "id": "4M92_eHqyvtf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_sequence(string: str, word_index: dict,\n",
        "                max_length: int = 88) -> np.ndarray :\n",
        "    \"\"\"\n",
        "    Convert the sentence into sequence of integers, refers from the word index.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. string : str -> The sentence to be converted.\n",
        "\n",
        "    2. word_index : dict -> The word index dictionary.\n",
        "\n",
        "    3. max_length : int -> The maximum length of the sequences. Must match the max_length for the model.\n",
        "\n",
        "    Return : np.ndarray -> The sequence in numpy array form.\n",
        "    \"\"\"\n",
        "    sentence = remove_punc(string).lower()\n",
        "    sentence_arr = sentence.split()\n",
        "    words = word_index.keys()\n",
        "    sequence = []\n",
        "    for w in sentence_arr :\n",
        "        if w in words :\n",
        "            sequence.append(word_index[w])\n",
        "        else :\n",
        "            sequence.append(1)\n",
        "\n",
        "    if len(sequence) < max_length :\n",
        "        num_zero = max_length - len(sequence)\n",
        "        sequence += [0]*num_zero\n",
        "    else :\n",
        "        sequence = sequence[:max_length]\n",
        "\n",
        "    sequence = np.array(sequence).reshape((1, -1))\n",
        "    return sequence"
      ],
      "metadata": {
        "id": "tX18xiuEyz6-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model: tf.keras.models.Model, sequence: np.ndarray,\n",
        "            label_dict: dict) -> str :\n",
        "    \"\"\"\n",
        "    Predict the class using the model.\n",
        "\n",
        "    Params :\n",
        "\n",
        "    1. model : tf.keras.models.Model\n",
        "\n",
        "    2. sequence : np.ndarray -> The sequence of integers, resulted from the conversion based on the word index.\n",
        "\n",
        "    3. label_dict : dict -> The label dictionary that stores integers as its keys and the class string as its values.\n",
        "\n",
        "    Return : str -> The predicted class.\n",
        "    \"\"\"\n",
        "    result = model.predict(sequence)\n",
        "    class_pred = np.argmax(result)\n",
        "    prediction = label_dict[class_pred]\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "zZsqo0Jey0wO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main() :\n",
        "    sentence = input(\"\\nMasukkan gejala Anda : \").strip()\n",
        "\n",
        "    model = get_model()\n",
        "    word_index = get_word_index()\n",
        "    label_dict = get_label_dict()\n",
        "\n",
        "    sequence = to_sequence(sentence, word_index)\n",
        "    pred = predict(model, sequence, label_dict)\n",
        "\n",
        "    print(\"\\nPrediction : {}\".format(pred))"
      ],
      "metadata": {
        "id": "I9J5PcS6y2nk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Inference"
      ],
      "metadata": {
        "id": "NypfPuEBy--o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = True\n",
        "while run :\n",
        "  main()\n",
        "  run_ans = input(\"Ulang Program? (Y/N) \")\n",
        "  if run_ans.lower() != 'y' :\n",
        "    break"
      ],
      "metadata": {
        "id": "C2T9BKZNzAR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ac1c26-2f71-4678-d2a1-eb55e9f770ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Masukkan gejala Anda : saya merasa mual, heartburn, jantung berdebar, dan nyeri di ulu hati\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "\n",
            "Prediction : GERD\n",
            "Ulang Program? (Y/N) Y\n",
            "\n",
            "Masukkan gejala Anda : kulit kuning, nyeri di perut kanan atas, demam, dan urine gelap\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "\n",
            "Prediction : Batu Empedu\n",
            "Ulang Program? (Y/N) N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to TFjs"
      ],
      "metadata": {
        "id": "y-_I13nqDv7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "id": "0jfVZw1sDy3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter \\\n",
        "--input_format=keras \\\n",
        "/content/model.h5 \\\n",
        "/content/model"
      ],
      "metadata": {
        "id": "dg5c862REFsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r digestion_diseases_prediction_json.zip model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjiRPc9MEMQg",
        "outputId": "dae51ab5-4bd1-4d5f-8306-3241ca11b42c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/model.json (deflated 78%)\n",
            "  adding: model/group1-shard1of1.bin (deflated 8%)\n"
          ]
        }
      ]
    }
  ]
}