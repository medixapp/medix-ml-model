{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for disease prediction, with predefined symptoms using one hot encodi"
      ],
      "metadata": {
        "id": "kfN4ddqtkNA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "nyndW29FkTlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import json\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "i8jXi0WLZDhU"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "290fPrEzkbDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pencernaan_data():\n",
        "    \"\"\"\n",
        "    Get pandas.DataFrame for Pencernaan data.\n",
        "\n",
        "    Params: None\n",
        "\n",
        "    Return: pandas.DataFrame\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv('list_penyakit.csv')\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        df = pd.DataFrame()  # Return an empty DataFrame in case of an error\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "VzMLhevrf85x"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_data(dataframe, col_of_list, label_col, num_samples=5, n=5):\n",
        "    \"\"\"\n",
        "    Sample data from a DataFrame where one column contains lists and another contains labels.\n",
        "\n",
        "    Params:\n",
        "    - dataframe (pd.DataFrame): The input DataFrame.\n",
        "    - col_of_list (str): The column name which contains lists.\n",
        "    - label_col (str): The column name which contains labels.\n",
        "    - num_samples (int): Number of samples to generate for each record.\n",
        "    - n (int): Number of elements to sample from each list.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A new DataFrame with sampled data.\n",
        "    \"\"\"\n",
        "    samples, labels = [], []\n",
        "    col_of_list_index = dataframe.columns.to_list().index(col_of_list)\n",
        "    label_col_index = dataframe.columns.to_list().index(label_col)\n",
        "\n",
        "    for record_num in range(len(dataframe)):\n",
        "        record_list = dataframe.iloc[record_num, col_of_list_index]\n",
        "        record_label = dataframe.iloc[record_num, label_col_index]\n",
        "\n",
        "        # Ensure record_list is a list\n",
        "        if isinstance(record_list, str):\n",
        "            try:\n",
        "                record_list = ast.literal_eval(record_list)\n",
        "            except (ValueError, SyntaxError):\n",
        "                record_list = record_list.split(', ')\n",
        "        elif not isinstance(record_list, list):\n",
        "            record_list = list(record_list)\n",
        "\n",
        "        if len(record_list) >= n:\n",
        "            for _ in range(num_samples):\n",
        "                samples.append(np.random.choice(record_list, n, replace=False).tolist())\n",
        "                labels.append(record_label)\n",
        "        else:\n",
        "            for _ in range(num_samples):\n",
        "                samples.append(np.random.choice(record_list, len(record_list), replace=False).tolist())\n",
        "                labels.append(record_label)\n",
        "\n",
        "    new_df = pd.DataFrame(list(zip(samples, labels)), columns=[col_of_list, label_col])\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "XvG-OsivjWv7"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_multiple_n(dataframe, col_of_list, label_col, num_samples=20, n_values=[5, 4, 3, 2]):\n",
        "    \"\"\"\n",
        "    Create multiple sampled DataFrames for different values of n and concatenate them.\n",
        "\n",
        "    Params:\n",
        "    - dataframe (pd.DataFrame): The input DataFrame.\n",
        "    - col_of_list (str): The column name which contains lists.\n",
        "    - label_col (str): The column name which contains labels.\n",
        "    - num_samples (int): Number of samples to generate for each record.\n",
        "    - n_values (list of int): List of n values to use for sampling.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A concatenated DataFrame with all samples.\n",
        "    \"\"\"\n",
        "    sampled_dfs = [dataframe]\n",
        "\n",
        "    for n in n_values:\n",
        "        sampled_df = sample_data(dataframe, col_of_list, label_col, num_samples, n)\n",
        "        sampled_dfs.append(sampled_df)\n",
        "\n",
        "    concatenated_df = pd.concat(sampled_dfs).sort_values(by=[label_col]).reset_index(drop=True)\n",
        "    return concatenated_df"
      ],
      "metadata": {
        "id": "JG92epj7aWjj"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_symptoms(dataframe, symptoms_col, label_col):\n",
        "    \"\"\"\n",
        "    One-hot encoding the symptoms in the specified column and concatenate the result with the label column.\n",
        "\n",
        "    Params:\n",
        "    - dataframe (pd.DataFrame): The input DataFrame.\n",
        "    - symptoms_col (str): The column name containing the symptoms lists.\n",
        "    - label_col (str): The column name containing the labels.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame with one-hot encoded symptoms with the original labels.\n",
        "    \"\"\"\n",
        "    # Ensure symptoms elements are lists\n",
        "    dataframe[symptoms_col] = dataframe[symptoms_col].apply(\n",
        "        lambda x: x if isinstance(x, list) else x.tolist() if isinstance(x, np.ndarray) else x.split(', ')\n",
        "    )\n",
        "\n",
        "    # Get all unique symptoms\n",
        "    all_symptoms = sorted(list(set(sum(dataframe[symptoms_col], []))))\n",
        "\n",
        "    # Create a binary matrix for one-hot encoding\n",
        "    binary_matrix = {symptom: dataframe[symptoms_col].apply(lambda x: int(symptom in x)) for symptom in all_symptoms}\n",
        "\n",
        "    # Create a one-hot encoded DataFrame\n",
        "    one_hot_encoded_df = pd.DataFrame(binary_matrix)\n",
        "\n",
        "    # Concatenate the label column with the one-hot encoded DataFrame\n",
        "    df_final = pd.concat([dataframe[label_col], one_hot_encoded_df], axis=1)\n",
        "\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "94Om6ITzgBRB"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def shuffle_and_split(dataframe, label_col, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Shuffle the DataFrame and split it into train and test sets, ensuring that each class in the label column\n",
        "    exist in both sets.\n",
        "\n",
        "    Params:\n",
        "    - dataframe (pd.DataFrame): The input DataFrame.\n",
        "    - label_col (str): The column name containing the class labels.\n",
        "    - test_size (float): The proportion of the dataset to include in the test split.\n",
        "    - random_state (int): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - df_train (pd.DataFrame): The training set.\n",
        "    - df_test (pd.DataFrame): The test set.\n",
        "    \"\"\"\n",
        "    # Shuffle the DataFrame\n",
        "    df_shuffled = dataframe.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    df_train, df_test = train_test_split(df_shuffled, test_size=test_size, stratify=df_shuffled[label_col],\n",
        "                                         random_state=random_state)\n",
        "\n",
        "    return df_train, df_test"
      ],
      "metadata": {
        "id": "6qGJik4OgOYq"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_and_labels(df_train, df_test, label_col):\n",
        "    \"\"\"\n",
        "    Extract features (x) and labels (y) from training and testing DataFrames.\n",
        "\n",
        "    Params:\n",
        "    - df_train (pd.DataFrame): The training DataFrame.\n",
        "    - df_test (pd.DataFrame): The testing DataFrame.\n",
        "    - label_col (str): The column name containing the labels.\n",
        "\n",
        "    Returns:\n",
        "    - x_train (np.ndarray): Training features.\n",
        "    - y_train (pd.Series): Training labels.\n",
        "    - x_test (np.ndarray): Testing features.\n",
        "    - y_test (pd.Series): Testing labels.\n",
        "    \"\"\"\n",
        "    # Extract features and labels\n",
        "    x_train, y_train = df_train.drop(label_col, axis=1), df_train[label_col]\n",
        "    x_test, y_test = df_test.drop(label_col, axis=1), df_test[label_col]\n",
        "\n",
        "    # Convert features to int32\n",
        "    x_train, x_test = x_train.astype('int32'), x_test.astype('int32')\n",
        "\n",
        "    # Convert features to numpy arrays\n",
        "    x_train, x_test = x_train.values, x_test.values\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "0yVWX4swgPA5"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_labels(y_train, y_test):\n",
        "    \"\"\"\n",
        "    Encode and one-hot encode labels for training and testing sets.\n",
        "\n",
        "    Params:\n",
        "    - y_train (pd.Series or np.ndarray): Training labels.\n",
        "    - y_test (pd.Series or np.ndarray): Testing labels.\n",
        "\n",
        "    Returns:\n",
        "    - y_train_encoded (np.ndarray): Encoded and one-hot encoded training labels.\n",
        "    - y_test_encoded (np.ndarray): Encoded and one-hot encoded testing labels.\n",
        "    - label_encoder (LabelEncoder): Fitted LabelEncoder instance.\n",
        "    \"\"\"\n",
        "    # Initialize LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    # Fit and transform the training labels\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Transform the testing labels\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    y_train_encoded = tf.cast(to_categorical(y_train_encoded), tf.int32).numpy()\n",
        "    y_test_encoded = tf.cast(to_categorical(y_test_encoded), tf.int32).numpy()\n",
        "\n",
        "    return y_train_encoded, y_test_encoded, label_encoder"
      ],
      "metadata": {
        "id": "3apkRZx7gRyc"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "lPDpFqgJKA_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_model(num_classes):\n",
        "    \"\"\"\n",
        "    Build a Keras Sequential model.\n",
        "\n",
        "    Params:\n",
        "    - num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "    - model (tf.keras.Model): Uncompiled Keras model.\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(14, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "OBYDDznP1OWZ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_model(model, learning_rate=0.0001):\n",
        "    \"\"\"\n",
        "    Compile a Keras model.\n",
        "\n",
        "    Params:\n",
        "    - model (tf.keras.Model): The Keras model to compile.\n",
        "    - learning_rate (float): Learning rate for the optimizer.\n",
        "\n",
        "    Returns:\n",
        "    - model (tf.keras.Model): Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "J2IKYbyi2X6M"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, x_train, y_train, x_val, y_val, epochs=1000):\n",
        "    \"\"\"\n",
        "    Train a Keras model.\n",
        "\n",
        "    Params:\n",
        "    - model (tf.keras.Model): The Keras model to train.\n",
        "    - x_train (np.ndarray): Training features.\n",
        "    - y_train (np.ndarray): Training labels.\n",
        "    - x_val (np.ndarray): Validation features.\n",
        "    - y_val (np.ndarray): Validation labels.\n",
        "    - epochs (int): Number of epochs to train.\n",
        "\n",
        "    Returns:\n",
        "    - history (tf.keras.callbacks.History): History object containing training history.\n",
        "    \"\"\"\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
        "    return history"
      ],
      "metadata": {
        "id": "QO997nKW2bVe"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model: tf.keras.models.Model) :\n",
        "    model.save('predefined_model.h5')"
      ],
      "metadata": {
        "id": "OyJciukq2eY_"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "6tfIQxL3MMj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training():\n",
        "\n",
        "    # Get symptoms and diseases data\n",
        "    df = get_pencernaan_data()\n",
        "\n",
        "    # Get sampled data\n",
        "    df_sampled = sample_multiple_n(df, \"Gejala\", \"Penyakit\", num_samples=20)\n",
        "\n",
        "    # Get one hot encoded data\n",
        "    df_final = one_hot_encode_symptoms(df_sampled, 'Gejala', 'Penyakit')\n",
        "\n",
        "    # Get training and testing set\n",
        "    df_train, df_test = shuffle_and_split(df_final, 'Penyakit', test_size=0.2)\n",
        "\n",
        "    # Get extracted feature and label of training and testing data\n",
        "    x_train, y_train, x_test, y_test = extract_features_and_labels(df_train, df_test, 'Penyakit')\n",
        "\n",
        "    # Encode labels\n",
        "    y_train_encoded, y_test_encoded, label_encoder = encode_labels(y_train, y_test)\n",
        "\n",
        "    # Determine input shape and number of classes\n",
        "    num_classes = y_train_encoded.shape[1]\n",
        "\n",
        "    # Define the model\n",
        "    model = build_model(num_classes)\n",
        "\n",
        "    # Compile the model\n",
        "    model = compile_model(model)\n",
        "\n",
        "    # Train the model\n",
        "    _ = train_model(model, x_train, y_train_encoded, x_test, y_test_encoded)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(x_test, y_test_encoded)\n",
        "    print(f'Test accuracy: {accuracy:.2f}')\n",
        "    print(f'Test loss: {loss:.2f}')\n",
        "\n",
        "    save_model(model)\n",
        "\n",
        "    # Get unique symptoms and labels\n",
        "    unique_symptoms = set()\n",
        "    for symptoms in df['Gejala'].str.split(', '):\n",
        "        unique_symptoms.update(symptoms)\n",
        "    with open('all_symptoms.txt', 'w') as txt_file:\n",
        "        txt_file.write(', '.join(sorted(unique_symptoms)))\n",
        "\n",
        "    unique_labels = df['Penyakit'].unique().tolist()\n",
        "    class_dict = {i: label for i, label in enumerate(unique_labels)}\n",
        "    with open('class_dict.json', 'w') as json_file:\n",
        "        json.dump(class_dict, json_file)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    run_training()"
      ],
      "metadata": {
        "id": "dNf29ne3Jxsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8892039-7573-4dc7-8c64-e5fa581d55d7"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "29/29 [==============================] - 1s 9ms/step - loss: 2.6256 - accuracy: 0.0750 - val_loss: 2.6117 - val_accuracy: 0.1057\n",
            "Epoch 2/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6131 - accuracy: 0.0816 - val_loss: 2.5965 - val_accuracy: 0.1189\n",
            "Epoch 3/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5997 - accuracy: 0.1080 - val_loss: 2.5816 - val_accuracy: 0.1278\n",
            "Epoch 4/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5803 - accuracy: 0.1213 - val_loss: 2.5664 - val_accuracy: 0.1454\n",
            "Epoch 5/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5641 - accuracy: 0.1544 - val_loss: 2.5506 - val_accuracy: 0.1938\n",
            "Epoch 6/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5461 - accuracy: 0.1808 - val_loss: 2.5344 - val_accuracy: 0.2467\n",
            "Epoch 7/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5338 - accuracy: 0.2095 - val_loss: 2.5174 - val_accuracy: 0.3216\n",
            "Epoch 8/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.5134 - accuracy: 0.2547 - val_loss: 2.4996 - val_accuracy: 0.3612\n",
            "Epoch 9/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4937 - accuracy: 0.2999 - val_loss: 2.4810 - val_accuracy: 0.3877\n",
            "Epoch 10/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.4725 - accuracy: 0.3252 - val_loss: 2.4615 - val_accuracy: 0.4229\n",
            "Epoch 11/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4500 - accuracy: 0.3495 - val_loss: 2.4405 - val_accuracy: 0.4405\n",
            "Epoch 12/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4314 - accuracy: 0.3870 - val_loss: 2.4184 - val_accuracy: 0.4581\n",
            "Epoch 13/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.4116 - accuracy: 0.3969 - val_loss: 2.3956 - val_accuracy: 0.4714\n",
            "Epoch 14/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3835 - accuracy: 0.4267 - val_loss: 2.3715 - val_accuracy: 0.4934\n",
            "Epoch 15/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3585 - accuracy: 0.4576 - val_loss: 2.3456 - val_accuracy: 0.5022\n",
            "Epoch 16/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3349 - accuracy: 0.4542 - val_loss: 2.3179 - val_accuracy: 0.5154\n",
            "Epoch 17/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.3079 - accuracy: 0.4752 - val_loss: 2.2893 - val_accuracy: 0.5286\n",
            "Epoch 18/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2749 - accuracy: 0.5006 - val_loss: 2.2595 - val_accuracy: 0.5551\n",
            "Epoch 19/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.2461 - accuracy: 0.5061 - val_loss: 2.2277 - val_accuracy: 0.5727\n",
            "Epoch 20/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.2160 - accuracy: 0.5513 - val_loss: 2.1944 - val_accuracy: 0.5815\n",
            "Epoch 21/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1854 - accuracy: 0.5546 - val_loss: 2.1598 - val_accuracy: 0.5947\n",
            "Epoch 22/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1435 - accuracy: 0.6020 - val_loss: 2.1236 - val_accuracy: 0.6211\n",
            "Epoch 23/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.1147 - accuracy: 0.5854 - val_loss: 2.0861 - val_accuracy: 0.6300\n",
            "Epoch 24/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.0762 - accuracy: 0.6031 - val_loss: 2.0474 - val_accuracy: 0.6432\n",
            "Epoch 25/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 2.0354 - accuracy: 0.6185 - val_loss: 2.0075 - val_accuracy: 0.6432\n",
            "Epoch 26/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 2.0047 - accuracy: 0.6031 - val_loss: 1.9669 - val_accuracy: 0.6476\n",
            "Epoch 27/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.9576 - accuracy: 0.6450 - val_loss: 1.9246 - val_accuracy: 0.6652\n",
            "Epoch 28/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.9115 - accuracy: 0.6703 - val_loss: 1.8821 - val_accuracy: 0.6784\n",
            "Epoch 29/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.8858 - accuracy: 0.6615 - val_loss: 1.8390 - val_accuracy: 0.6872\n",
            "Epoch 30/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.8388 - accuracy: 0.6703 - val_loss: 1.7953 - val_accuracy: 0.6960\n",
            "Epoch 31/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.8014 - accuracy: 0.6637 - val_loss: 1.7509 - val_accuracy: 0.7004\n",
            "Epoch 32/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.7432 - accuracy: 0.6990 - val_loss: 1.7058 - val_accuracy: 0.7093\n",
            "Epoch 33/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.7083 - accuracy: 0.6946 - val_loss: 1.6605 - val_accuracy: 0.7181\n",
            "Epoch 34/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.6579 - accuracy: 0.7387 - val_loss: 1.6149 - val_accuracy: 0.7357\n",
            "Epoch 35/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.6241 - accuracy: 0.7244 - val_loss: 1.5695 - val_accuracy: 0.7489\n",
            "Epoch 36/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.5676 - accuracy: 0.7607 - val_loss: 1.5247 - val_accuracy: 0.7621\n",
            "Epoch 37/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.5311 - accuracy: 0.7552 - val_loss: 1.4796 - val_accuracy: 0.7797\n",
            "Epoch 38/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.4751 - accuracy: 0.7839 - val_loss: 1.4344 - val_accuracy: 0.8018\n",
            "Epoch 39/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.4511 - accuracy: 0.7630 - val_loss: 1.3906 - val_accuracy: 0.8106\n",
            "Epoch 40/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.4100 - accuracy: 0.7872 - val_loss: 1.3475 - val_accuracy: 0.8106\n",
            "Epoch 41/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.3692 - accuracy: 0.7938 - val_loss: 1.3050 - val_accuracy: 0.8150\n",
            "Epoch 42/1000\n",
            "29/29 [==============================] - 0s 6ms/step - loss: 1.3324 - accuracy: 0.8060 - val_loss: 1.2631 - val_accuracy: 0.8414\n",
            "Epoch 43/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.2901 - accuracy: 0.8049 - val_loss: 1.2221 - val_accuracy: 0.8546\n",
            "Epoch 44/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 1.2494 - accuracy: 0.7993 - val_loss: 1.1824 - val_accuracy: 0.8634\n",
            "Epoch 45/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.2293 - accuracy: 0.8214 - val_loss: 1.1447 - val_accuracy: 0.8634\n",
            "Epoch 46/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.1874 - accuracy: 0.8203 - val_loss: 1.1070 - val_accuracy: 0.8767\n",
            "Epoch 47/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.1347 - accuracy: 0.8291 - val_loss: 1.0706 - val_accuracy: 0.8943\n",
            "Epoch 48/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.0985 - accuracy: 0.8335 - val_loss: 1.0353 - val_accuracy: 0.8943\n",
            "Epoch 49/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.0848 - accuracy: 0.8269 - val_loss: 1.0018 - val_accuracy: 0.8899\n",
            "Epoch 50/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.0415 - accuracy: 0.8313 - val_loss: 0.9693 - val_accuracy: 0.8943\n",
            "Epoch 51/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9920 - accuracy: 0.8798 - val_loss: 0.9368 - val_accuracy: 0.8987\n",
            "Epoch 52/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.9884 - accuracy: 0.8578 - val_loss: 0.9064 - val_accuracy: 0.9031\n",
            "Epoch 53/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.9458 - accuracy: 0.8677 - val_loss: 0.8767 - val_accuracy: 0.9163\n",
            "Epoch 54/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.9149 - accuracy: 0.8798 - val_loss: 0.8481 - val_accuracy: 0.9207\n",
            "Epoch 55/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.8834 - accuracy: 0.8710 - val_loss: 0.8212 - val_accuracy: 0.9163\n",
            "Epoch 56/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.8730 - accuracy: 0.8699 - val_loss: 0.7951 - val_accuracy: 0.9163\n",
            "Epoch 57/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.8470 - accuracy: 0.8699 - val_loss: 0.7709 - val_accuracy: 0.9163\n",
            "Epoch 58/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.8325 - accuracy: 0.8655 - val_loss: 0.7467 - val_accuracy: 0.9251\n",
            "Epoch 59/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.7972 - accuracy: 0.8831 - val_loss: 0.7240 - val_accuracy: 0.9295\n",
            "Epoch 60/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.7766 - accuracy: 0.8842 - val_loss: 0.7020 - val_accuracy: 0.9295\n",
            "Epoch 61/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.7491 - accuracy: 0.8776 - val_loss: 0.6816 - val_accuracy: 0.9295\n",
            "Epoch 62/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.7351 - accuracy: 0.8942 - val_loss: 0.6619 - val_accuracy: 0.9295\n",
            "Epoch 63/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.7238 - accuracy: 0.8842 - val_loss: 0.6424 - val_accuracy: 0.9339\n",
            "Epoch 64/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6996 - accuracy: 0.8875 - val_loss: 0.6247 - val_accuracy: 0.9339\n",
            "Epoch 65/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.9096 - val_loss: 0.6069 - val_accuracy: 0.9339\n",
            "Epoch 66/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.8897 - val_loss: 0.5900 - val_accuracy: 0.9339\n",
            "Epoch 67/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6572 - accuracy: 0.8864 - val_loss: 0.5740 - val_accuracy: 0.9339\n",
            "Epoch 68/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6377 - accuracy: 0.8920 - val_loss: 0.5595 - val_accuracy: 0.9339\n",
            "Epoch 69/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6237 - accuracy: 0.8875 - val_loss: 0.5451 - val_accuracy: 0.9339\n",
            "Epoch 70/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6225 - accuracy: 0.8897 - val_loss: 0.5301 - val_accuracy: 0.9339\n",
            "Epoch 71/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.8964 - val_loss: 0.5161 - val_accuracy: 0.9339\n",
            "Epoch 72/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.5724 - accuracy: 0.8842 - val_loss: 0.5035 - val_accuracy: 0.9339\n",
            "Epoch 73/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.5694 - accuracy: 0.9063 - val_loss: 0.4908 - val_accuracy: 0.9339\n",
            "Epoch 74/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.5350 - accuracy: 0.9041 - val_loss: 0.4789 - val_accuracy: 0.9339\n",
            "Epoch 75/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.5219 - accuracy: 0.9096 - val_loss: 0.4684 - val_accuracy: 0.9339\n",
            "Epoch 76/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.5248 - accuracy: 0.9151 - val_loss: 0.4574 - val_accuracy: 0.9339\n",
            "Epoch 77/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.5268 - accuracy: 0.9118 - val_loss: 0.4471 - val_accuracy: 0.9339\n",
            "Epoch 78/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.5240 - accuracy: 0.8964 - val_loss: 0.4375 - val_accuracy: 0.9339\n",
            "Epoch 79/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.9074 - val_loss: 0.4278 - val_accuracy: 0.9339\n",
            "Epoch 80/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.9228 - val_loss: 0.4185 - val_accuracy: 0.9339\n",
            "Epoch 81/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.4689 - accuracy: 0.9195 - val_loss: 0.4094 - val_accuracy: 0.9339\n",
            "Epoch 82/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.4739 - accuracy: 0.8986 - val_loss: 0.4005 - val_accuracy: 0.9339\n",
            "Epoch 83/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.8986 - val_loss: 0.3926 - val_accuracy: 0.9339\n",
            "Epoch 84/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9030 - val_loss: 0.3850 - val_accuracy: 0.9339\n",
            "Epoch 85/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.9107 - val_loss: 0.3776 - val_accuracy: 0.9339\n",
            "Epoch 86/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.9140 - val_loss: 0.3698 - val_accuracy: 0.9339\n",
            "Epoch 87/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.9250 - val_loss: 0.3640 - val_accuracy: 0.9339\n",
            "Epoch 88/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4168 - accuracy: 0.9151 - val_loss: 0.3572 - val_accuracy: 0.9339\n",
            "Epoch 89/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.9096 - val_loss: 0.3517 - val_accuracy: 0.9383\n",
            "Epoch 90/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.9250 - val_loss: 0.3453 - val_accuracy: 0.9295\n",
            "Epoch 91/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.9217 - val_loss: 0.3400 - val_accuracy: 0.9295\n",
            "Epoch 92/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.9107 - val_loss: 0.3338 - val_accuracy: 0.9339\n",
            "Epoch 93/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3909 - accuracy: 0.9206 - val_loss: 0.3281 - val_accuracy: 0.9339\n",
            "Epoch 94/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.9294 - val_loss: 0.3226 - val_accuracy: 0.9383\n",
            "Epoch 95/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.9272 - val_loss: 0.3177 - val_accuracy: 0.9383\n",
            "Epoch 96/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.9206 - val_loss: 0.3133 - val_accuracy: 0.9383\n",
            "Epoch 97/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.9140 - val_loss: 0.3091 - val_accuracy: 0.9383\n",
            "Epoch 98/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.9184 - val_loss: 0.3031 - val_accuracy: 0.9383\n",
            "Epoch 99/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.9294 - val_loss: 0.2988 - val_accuracy: 0.9383\n",
            "Epoch 100/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.9239 - val_loss: 0.2944 - val_accuracy: 0.9383\n",
            "Epoch 101/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.9217 - val_loss: 0.2906 - val_accuracy: 0.9383\n",
            "Epoch 102/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.9195 - val_loss: 0.2864 - val_accuracy: 0.9383\n",
            "Epoch 103/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.9327 - val_loss: 0.2823 - val_accuracy: 0.9383\n",
            "Epoch 104/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.9383 - val_loss: 0.2788 - val_accuracy: 0.9383\n",
            "Epoch 105/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.9206 - val_loss: 0.2754 - val_accuracy: 0.9383\n",
            "Epoch 106/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.9129 - val_loss: 0.2717 - val_accuracy: 0.9383\n",
            "Epoch 107/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.9206 - val_loss: 0.2682 - val_accuracy: 0.9383\n",
            "Epoch 108/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3150 - accuracy: 0.9239 - val_loss: 0.2637 - val_accuracy: 0.9383\n",
            "Epoch 109/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.9261 - val_loss: 0.2602 - val_accuracy: 0.9383\n",
            "Epoch 110/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.9272 - val_loss: 0.2562 - val_accuracy: 0.9383\n",
            "Epoch 111/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3096 - accuracy: 0.9261 - val_loss: 0.2534 - val_accuracy: 0.9383\n",
            "Epoch 112/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2907 - accuracy: 0.9294 - val_loss: 0.2508 - val_accuracy: 0.9383\n",
            "Epoch 113/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.9305 - val_loss: 0.2481 - val_accuracy: 0.9383\n",
            "Epoch 114/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.9405 - val_loss: 0.2461 - val_accuracy: 0.9383\n",
            "Epoch 115/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2858 - accuracy: 0.9327 - val_loss: 0.2429 - val_accuracy: 0.9383\n",
            "Epoch 116/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.9350 - val_loss: 0.2406 - val_accuracy: 0.9383\n",
            "Epoch 117/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.9327 - val_loss: 0.2377 - val_accuracy: 0.9383\n",
            "Epoch 118/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2915 - accuracy: 0.9316 - val_loss: 0.2355 - val_accuracy: 0.9427\n",
            "Epoch 119/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.9316 - val_loss: 0.2329 - val_accuracy: 0.9427\n",
            "Epoch 120/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2777 - accuracy: 0.9316 - val_loss: 0.2300 - val_accuracy: 0.9427\n",
            "Epoch 121/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2689 - accuracy: 0.9272 - val_loss: 0.2281 - val_accuracy: 0.9427\n",
            "Epoch 122/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9383 - val_loss: 0.2263 - val_accuracy: 0.9427\n",
            "Epoch 123/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2674 - accuracy: 0.9338 - val_loss: 0.2238 - val_accuracy: 0.9427\n",
            "Epoch 124/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2644 - accuracy: 0.9361 - val_loss: 0.2221 - val_accuracy: 0.9427\n",
            "Epoch 125/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2797 - accuracy: 0.9250 - val_loss: 0.2207 - val_accuracy: 0.9427\n",
            "Epoch 126/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9283 - val_loss: 0.2188 - val_accuracy: 0.9427\n",
            "Epoch 127/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2782 - accuracy: 0.9261 - val_loss: 0.2166 - val_accuracy: 0.9427\n",
            "Epoch 128/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2509 - accuracy: 0.9361 - val_loss: 0.2146 - val_accuracy: 0.9427\n",
            "Epoch 129/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.9283 - val_loss: 0.2120 - val_accuracy: 0.9427\n",
            "Epoch 130/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.9294 - val_loss: 0.2104 - val_accuracy: 0.9427\n",
            "Epoch 131/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9405 - val_loss: 0.2100 - val_accuracy: 0.9427\n",
            "Epoch 132/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2568 - accuracy: 0.9372 - val_loss: 0.2086 - val_accuracy: 0.9427\n",
            "Epoch 133/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2559 - accuracy: 0.9283 - val_loss: 0.2069 - val_accuracy: 0.9427\n",
            "Epoch 134/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.9372 - val_loss: 0.2049 - val_accuracy: 0.9427\n",
            "Epoch 135/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2475 - accuracy: 0.9327 - val_loss: 0.2028 - val_accuracy: 0.9427\n",
            "Epoch 136/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.9361 - val_loss: 0.2016 - val_accuracy: 0.9427\n",
            "Epoch 137/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9416 - val_loss: 0.2004 - val_accuracy: 0.9427\n",
            "Epoch 138/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.9338 - val_loss: 0.1987 - val_accuracy: 0.9471\n",
            "Epoch 139/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9327 - val_loss: 0.1965 - val_accuracy: 0.9471\n",
            "Epoch 140/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2408 - accuracy: 0.9361 - val_loss: 0.1956 - val_accuracy: 0.9471\n",
            "Epoch 141/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.9361 - val_loss: 0.1946 - val_accuracy: 0.9427\n",
            "Epoch 142/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2390 - accuracy: 0.9338 - val_loss: 0.1927 - val_accuracy: 0.9471\n",
            "Epoch 143/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.9338 - val_loss: 0.1918 - val_accuracy: 0.9471\n",
            "Epoch 144/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2335 - accuracy: 0.9327 - val_loss: 0.1898 - val_accuracy: 0.9471\n",
            "Epoch 145/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9361 - val_loss: 0.1878 - val_accuracy: 0.9427\n",
            "Epoch 146/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9383 - val_loss: 0.1870 - val_accuracy: 0.9471\n",
            "Epoch 147/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9283 - val_loss: 0.1858 - val_accuracy: 0.9471\n",
            "Epoch 148/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2314 - accuracy: 0.9250 - val_loss: 0.1851 - val_accuracy: 0.9471\n",
            "Epoch 149/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2157 - accuracy: 0.9427 - val_loss: 0.1846 - val_accuracy: 0.9515\n",
            "Epoch 150/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2135 - accuracy: 0.9493 - val_loss: 0.1836 - val_accuracy: 0.9515\n",
            "Epoch 151/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2151 - accuracy: 0.9405 - val_loss: 0.1817 - val_accuracy: 0.9515\n",
            "Epoch 152/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9327 - val_loss: 0.1812 - val_accuracy: 0.9515\n",
            "Epoch 153/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2248 - accuracy: 0.9294 - val_loss: 0.1802 - val_accuracy: 0.9515\n",
            "Epoch 154/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2253 - accuracy: 0.9416 - val_loss: 0.1790 - val_accuracy: 0.9515\n",
            "Epoch 155/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2128 - accuracy: 0.9383 - val_loss: 0.1783 - val_accuracy: 0.9515\n",
            "Epoch 156/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2085 - accuracy: 0.9383 - val_loss: 0.1774 - val_accuracy: 0.9515\n",
            "Epoch 157/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9338 - val_loss: 0.1764 - val_accuracy: 0.9515\n",
            "Epoch 158/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2145 - accuracy: 0.9350 - val_loss: 0.1753 - val_accuracy: 0.9515\n",
            "Epoch 159/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.9383 - val_loss: 0.1746 - val_accuracy: 0.9515\n",
            "Epoch 160/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2148 - accuracy: 0.9361 - val_loss: 0.1731 - val_accuracy: 0.9515\n",
            "Epoch 161/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9361 - val_loss: 0.1721 - val_accuracy: 0.9515\n",
            "Epoch 162/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9449 - val_loss: 0.1708 - val_accuracy: 0.9515\n",
            "Epoch 163/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.2042 - accuracy: 0.9416 - val_loss: 0.1700 - val_accuracy: 0.9471\n",
            "Epoch 164/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9515 - val_loss: 0.1691 - val_accuracy: 0.9471\n",
            "Epoch 165/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1988 - accuracy: 0.9405 - val_loss: 0.1685 - val_accuracy: 0.9471\n",
            "Epoch 166/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9327 - val_loss: 0.1677 - val_accuracy: 0.9471\n",
            "Epoch 167/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9394 - val_loss: 0.1668 - val_accuracy: 0.9515\n",
            "Epoch 168/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.2043 - accuracy: 0.9350 - val_loss: 0.1671 - val_accuracy: 0.9471\n",
            "Epoch 169/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9338 - val_loss: 0.1655 - val_accuracy: 0.9471\n",
            "Epoch 170/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1942 - accuracy: 0.9427 - val_loss: 0.1643 - val_accuracy: 0.9515\n",
            "Epoch 171/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9416 - val_loss: 0.1644 - val_accuracy: 0.9515\n",
            "Epoch 172/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1963 - accuracy: 0.9427 - val_loss: 0.1628 - val_accuracy: 0.9515\n",
            "Epoch 173/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9338 - val_loss: 0.1617 - val_accuracy: 0.9515\n",
            "Epoch 174/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9294 - val_loss: 0.1610 - val_accuracy: 0.9515\n",
            "Epoch 175/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9427 - val_loss: 0.1616 - val_accuracy: 0.9515\n",
            "Epoch 176/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9394 - val_loss: 0.1601 - val_accuracy: 0.9515\n",
            "Epoch 177/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1883 - accuracy: 0.9372 - val_loss: 0.1589 - val_accuracy: 0.9515\n",
            "Epoch 178/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9372 - val_loss: 0.1592 - val_accuracy: 0.9515\n",
            "Epoch 179/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1925 - accuracy: 0.9405 - val_loss: 0.1578 - val_accuracy: 0.9515\n",
            "Epoch 180/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9327 - val_loss: 0.1571 - val_accuracy: 0.9515\n",
            "Epoch 181/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9361 - val_loss: 0.1567 - val_accuracy: 0.9559\n",
            "Epoch 182/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1981 - accuracy: 0.9372 - val_loss: 0.1556 - val_accuracy: 0.9559\n",
            "Epoch 183/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1885 - accuracy: 0.9372 - val_loss: 0.1549 - val_accuracy: 0.9559\n",
            "Epoch 184/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9372 - val_loss: 0.1545 - val_accuracy: 0.9559\n",
            "Epoch 185/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9416 - val_loss: 0.1541 - val_accuracy: 0.9559\n",
            "Epoch 186/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9394 - val_loss: 0.1531 - val_accuracy: 0.9515\n",
            "Epoch 187/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9394 - val_loss: 0.1520 - val_accuracy: 0.9559\n",
            "Epoch 188/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9471 - val_loss: 0.1518 - val_accuracy: 0.9515\n",
            "Epoch 189/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1766 - accuracy: 0.9438 - val_loss: 0.1514 - val_accuracy: 0.9559\n",
            "Epoch 190/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.9482 - val_loss: 0.1516 - val_accuracy: 0.9515\n",
            "Epoch 191/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9416 - val_loss: 0.1508 - val_accuracy: 0.9559\n",
            "Epoch 192/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9526 - val_loss: 0.1500 - val_accuracy: 0.9559\n",
            "Epoch 193/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9372 - val_loss: 0.1495 - val_accuracy: 0.9559\n",
            "Epoch 194/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9438 - val_loss: 0.1498 - val_accuracy: 0.9515\n",
            "Epoch 195/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9493 - val_loss: 0.1492 - val_accuracy: 0.9515\n",
            "Epoch 196/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9416 - val_loss: 0.1483 - val_accuracy: 0.9515\n",
            "Epoch 197/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9405 - val_loss: 0.1485 - val_accuracy: 0.9559\n",
            "Epoch 198/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9493 - val_loss: 0.1483 - val_accuracy: 0.9559\n",
            "Epoch 199/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9394 - val_loss: 0.1479 - val_accuracy: 0.9559\n",
            "Epoch 200/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9471 - val_loss: 0.1463 - val_accuracy: 0.9559\n",
            "Epoch 201/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9416 - val_loss: 0.1457 - val_accuracy: 0.9559\n",
            "Epoch 202/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9449 - val_loss: 0.1452 - val_accuracy: 0.9559\n",
            "Epoch 203/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9416 - val_loss: 0.1439 - val_accuracy: 0.9559\n",
            "Epoch 204/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1748 - accuracy: 0.9438 - val_loss: 0.1441 - val_accuracy: 0.9559\n",
            "Epoch 205/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1727 - accuracy: 0.9416 - val_loss: 0.1443 - val_accuracy: 0.9559\n",
            "Epoch 206/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1794 - accuracy: 0.9416 - val_loss: 0.1442 - val_accuracy: 0.9559\n",
            "Epoch 207/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9361 - val_loss: 0.1438 - val_accuracy: 0.9515\n",
            "Epoch 208/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.9471 - val_loss: 0.1438 - val_accuracy: 0.9515\n",
            "Epoch 209/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1678 - accuracy: 0.9460 - val_loss: 0.1437 - val_accuracy: 0.9515\n",
            "Epoch 210/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9427 - val_loss: 0.1435 - val_accuracy: 0.9515\n",
            "Epoch 211/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9493 - val_loss: 0.1423 - val_accuracy: 0.9559\n",
            "Epoch 212/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1839 - accuracy: 0.9316 - val_loss: 0.1426 - val_accuracy: 0.9515\n",
            "Epoch 213/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9427 - val_loss: 0.1425 - val_accuracy: 0.9515\n",
            "Epoch 214/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9427 - val_loss: 0.1413 - val_accuracy: 0.9515\n",
            "Epoch 215/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9427 - val_loss: 0.1402 - val_accuracy: 0.9515\n",
            "Epoch 216/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9482 - val_loss: 0.1399 - val_accuracy: 0.9515\n",
            "Epoch 217/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.9449 - val_loss: 0.1397 - val_accuracy: 0.9559\n",
            "Epoch 218/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1650 - accuracy: 0.9449 - val_loss: 0.1397 - val_accuracy: 0.9515\n",
            "Epoch 219/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9416 - val_loss: 0.1401 - val_accuracy: 0.9515\n",
            "Epoch 220/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9504 - val_loss: 0.1401 - val_accuracy: 0.9515\n",
            "Epoch 221/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 0.9482 - val_loss: 0.1401 - val_accuracy: 0.9515\n",
            "Epoch 222/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9493 - val_loss: 0.1399 - val_accuracy: 0.9515\n",
            "Epoch 223/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9493 - val_loss: 0.1396 - val_accuracy: 0.9515\n",
            "Epoch 224/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9493 - val_loss: 0.1395 - val_accuracy: 0.9515\n",
            "Epoch 225/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9449 - val_loss: 0.1396 - val_accuracy: 0.9515\n",
            "Epoch 226/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9416 - val_loss: 0.1387 - val_accuracy: 0.9515\n",
            "Epoch 227/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.9526 - val_loss: 0.1382 - val_accuracy: 0.9515\n",
            "Epoch 228/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9460 - val_loss: 0.1383 - val_accuracy: 0.9515\n",
            "Epoch 229/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.9361 - val_loss: 0.1372 - val_accuracy: 0.9515\n",
            "Epoch 230/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9493 - val_loss: 0.1367 - val_accuracy: 0.9515\n",
            "Epoch 231/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1497 - accuracy: 0.9526 - val_loss: 0.1362 - val_accuracy: 0.9515\n",
            "Epoch 232/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.9405 - val_loss: 0.1357 - val_accuracy: 0.9515\n",
            "Epoch 233/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.9427 - val_loss: 0.1352 - val_accuracy: 0.9515\n",
            "Epoch 234/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9493 - val_loss: 0.1346 - val_accuracy: 0.9515\n",
            "Epoch 235/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.9460 - val_loss: 0.1351 - val_accuracy: 0.9515\n",
            "Epoch 236/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.9372 - val_loss: 0.1356 - val_accuracy: 0.9515\n",
            "Epoch 237/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9460 - val_loss: 0.1345 - val_accuracy: 0.9515\n",
            "Epoch 238/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9416 - val_loss: 0.1345 - val_accuracy: 0.9515\n",
            "Epoch 239/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.9471 - val_loss: 0.1335 - val_accuracy: 0.9559\n",
            "Epoch 240/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1533 - accuracy: 0.9449 - val_loss: 0.1331 - val_accuracy: 0.9559\n",
            "Epoch 241/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9482 - val_loss: 0.1330 - val_accuracy: 0.9559\n",
            "Epoch 242/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9537 - val_loss: 0.1328 - val_accuracy: 0.9559\n",
            "Epoch 243/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9537 - val_loss: 0.1330 - val_accuracy: 0.9515\n",
            "Epoch 244/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9482 - val_loss: 0.1329 - val_accuracy: 0.9559\n",
            "Epoch 245/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9471 - val_loss: 0.1321 - val_accuracy: 0.9559\n",
            "Epoch 246/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9427 - val_loss: 0.1312 - val_accuracy: 0.9559\n",
            "Epoch 247/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9471 - val_loss: 0.1309 - val_accuracy: 0.9559\n",
            "Epoch 248/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1486 - accuracy: 0.9471 - val_loss: 0.1316 - val_accuracy: 0.9559\n",
            "Epoch 249/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9493 - val_loss: 0.1316 - val_accuracy: 0.9515\n",
            "Epoch 250/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9559 - val_loss: 0.1306 - val_accuracy: 0.9559\n",
            "Epoch 251/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9493 - val_loss: 0.1308 - val_accuracy: 0.9559\n",
            "Epoch 252/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9515 - val_loss: 0.1311 - val_accuracy: 0.9559\n",
            "Epoch 253/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9416 - val_loss: 0.1303 - val_accuracy: 0.9559\n",
            "Epoch 254/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9493 - val_loss: 0.1307 - val_accuracy: 0.9559\n",
            "Epoch 255/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9383 - val_loss: 0.1298 - val_accuracy: 0.9559\n",
            "Epoch 256/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9383 - val_loss: 0.1304 - val_accuracy: 0.9515\n",
            "Epoch 257/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1485 - accuracy: 0.9427 - val_loss: 0.1306 - val_accuracy: 0.9515\n",
            "Epoch 258/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9493 - val_loss: 0.1304 - val_accuracy: 0.9515\n",
            "Epoch 259/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9482 - val_loss: 0.1296 - val_accuracy: 0.9515\n",
            "Epoch 260/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9493 - val_loss: 0.1300 - val_accuracy: 0.9515\n",
            "Epoch 261/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9559 - val_loss: 0.1293 - val_accuracy: 0.9515\n",
            "Epoch 262/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9460 - val_loss: 0.1299 - val_accuracy: 0.9515\n",
            "Epoch 263/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9570 - val_loss: 0.1296 - val_accuracy: 0.9515\n",
            "Epoch 264/1000\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9515 - val_loss: 0.1292 - val_accuracy: 0.9515\n",
            "Epoch 265/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9493 - val_loss: 0.1287 - val_accuracy: 0.9515\n",
            "Epoch 266/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9471 - val_loss: 0.1288 - val_accuracy: 0.9515\n",
            "Epoch 267/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9427 - val_loss: 0.1283 - val_accuracy: 0.9515\n",
            "Epoch 268/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.9471 - val_loss: 0.1283 - val_accuracy: 0.9515\n",
            "Epoch 269/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9504 - val_loss: 0.1284 - val_accuracy: 0.9515\n",
            "Epoch 270/1000\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9570 - val_loss: 0.1286 - val_accuracy: 0.9515\n",
            "Epoch 271/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1432 - accuracy: 0.9471 - val_loss: 0.1283 - val_accuracy: 0.9515\n",
            "Epoch 272/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9493 - val_loss: 0.1288 - val_accuracy: 0.9515\n",
            "Epoch 273/1000\n",
            "29/29 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9427 - val_loss: 0.1292 - val_accuracy: 0.9515\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9515\n",
            "Test accuracy: 0.95\n",
            "Test loss: 0.13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "7_ubSzDG3OLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('predefined_model.h5')\n",
        "\n",
        "# Initialize the label encoder and load the necessary files\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "with open('all_symptoms.txt', 'r') as symptoms_file:\n",
        "    all_symptoms = [symptom.strip() for symptom in symptoms_file.read().split(',')]\n",
        "\n",
        "with open('class_dict.json', 'r') as class_json:\n",
        "    class_dict = json.load(class_json)\n",
        "\n",
        "unique_classes = list(class_dict.values())\n",
        "label_encoder.fit(unique_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "38CuKWlO3Txd",
        "outputId": "4ff2fd5c-b915-4063-cdbe-64ed6c91238f"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input(input_symptoms, all_symptoms):\n",
        "    # Ensure only valid symptoms are considered\n",
        "    input_symptoms = [symptom.strip() for symptom in input_symptoms if symptom.strip() in all_symptoms]\n",
        "\n",
        "    # Create a binary representation of the symptoms\n",
        "    input_data = [int(symptom in input_symptoms) for symptom in all_symptoms][:68]\n",
        "\n",
        "    # Reshape to match the model's expected input shape\n",
        "    return np.array(input_data).reshape((1, -1))"
      ],
      "metadata": {
        "id": "d2xJxPnlJ0JE"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_display(input_data, model, label_encoder):\n",
        "    # Make predictions\n",
        "    predictions = model.predict(input_data)\n",
        "\n",
        "    # Get the index of the highest probability prediction\n",
        "    max_prob_index = np.argmax(predictions)\n",
        "\n",
        "    # Decode the predicted class from the index using the label_encoder\n",
        "    predicted_class = label_encoder.inverse_transform([max_prob_index])[0]\n",
        "\n",
        "    # Get the probability corresponding to the highest prediction\n",
        "    max_prob = predictions[0][max_prob_index]\n",
        "\n",
        "    # Display the predicted class and its probability\n",
        "    print(f\"Predicted Class: {predicted_class}, Probability: {max_prob:.2f}\")"
      ],
      "metadata": {
        "id": "_6qf_R1R3WWr"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Get input symptoms from the user\n",
        "    input_symptoms_str = input(\"Masukkan gejala yang dirasakan: \")\n",
        "    input_symptoms = [symptom.strip() for symptom in input_symptoms_str.split(',')]\n",
        "\n",
        "    # Preprocess input data and make predictions\n",
        "    input_data = preprocess_input(input_symptoms, all_symptoms)\n",
        "    predict_and_display(input_data, model, label_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUXqf1Dc3dUf",
        "outputId": "d1ad4e07-431e-4cf8-80ea-47355ff47432"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan gejala yang dirasakan: mual muntah, perut kembung, pusing, dehidrasi\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted Class: Diare, Probability: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to tfjs"
      ],
      "metadata": {
        "id": "TIAsWqv8MYcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ga7_kv-p4Jfh",
        "outputId": "6206f73f-e6a0-4776-9d75-2a7ee85b3b5c"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.20.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/89.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.8.4)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.4.0)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.1)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Collecting packaging~=23.1 (from tensorflowjs)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.25.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.8)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.0.3)\n",
            "Collecting tensorflow<3,>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
            "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
            "  Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading ydf-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py>=3.10.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes>=0.2.0 (from jax>=0.4.13->tensorflowjs)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting namex (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow<3,>=2.13.0->tensorflowjs)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.19.2)\n",
            "Installing collected packages: namex, ydf, wurlitzer, packaging, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow, tf-keras, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "  Attempting uninstall: tf-keras\n",
            "    Found existing installation: tf_keras 2.15.1\n",
            "    Uninstalling tf_keras-2.15.1:\n",
            "      Successfully uninstalled tf_keras-2.15.1\n",
            "Successfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 packaging-23.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-decision-forests-1.9.1 tensorflowjs-4.20.0 tf-keras-2.16.0 wurlitzer-3.1.1 ydf-0.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "81efff1af98b46608dcfadd1b3215dfd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter \\\n",
        "--input_format=keras \\\n",
        "/content/predefined_model.h5 \\\n",
        "/content/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmvXv5XIMcDH",
        "outputId": "ce0e6c7e-e390-405b-f480-7c1b6e320fed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-13 15:25:56.876081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-13 15:25:59.127788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r predefined_model_disease_prediction_json.zip model/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNa3d9BQMnRv",
        "outputId": "976a84a0-2319-4c67-e004-4233dabec802"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/group1-shard1of1.bin (deflated 8%)\n",
            "  adding: model/model.json (deflated 75%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6ONfJaMNMfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
